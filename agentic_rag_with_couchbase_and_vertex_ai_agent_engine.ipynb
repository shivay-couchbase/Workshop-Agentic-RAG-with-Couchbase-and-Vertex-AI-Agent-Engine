{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mj3aubG96l-"
      },
      "source": [
        "# Build AI Agents with Vertex AI Agent Engine and Couchbase\n",
        "\n",
        "[**Vertex AI Agent Engine**](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/) is a Google Cloud service that helps you build and scale AI agents in production. You can use the Agent Engine with **Couchbase Capella** and your preferred framework to build AI agents for a variety of use cases, including agentic **RAG** (Retrieval-Augmented Generation).\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Get Started\n",
        "\n",
        "This tutorial demonstrates how to use the **Agent Engine with Couchbase** to build a RAG agent that can answer questions about sample data. It uses **Couchbase Vector Search** with **LangChain** to implement the retrieval tools for the agent.\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before you begin, ensure you have the following:\n",
        "\n",
        "* A **Couchbase Capella cluster** in your preferred **region like Google Cloud**.\n",
        "  To create a new cluster, see: [Create a Free Cluster in Couchbase Capella](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
        "\n",
        "* A **Google Cloud project with Vertex AI enabled**.\n",
        "  To set up a project, refer to: [Set up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "\n",
        "* **Setup up Google Vertex AI Agent Engine:**.\n",
        "  [Follow these instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/set-up) to setup a Google Cloud project with billing enabled, have the required IAM permissions, enable the Vertex AI, Cloud Storage, Cloud Logging, Cloud Monitoring, and Cloud Trace APIs, set up a Cloud Storage bucket, and install the Vertex AI SDK for Python.\n",
        "\n",
        "---\n",
        "\n",
        "## Set up your Environment\n",
        "\n",
        "Create an interactive Python notebook by saving a file with the `.ipynb` extension using **Google Colab**. This notebook will allow you to run Python code snippets individually and build your agent step by step.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the Stage: Installing Necessary Libraries\n",
        "\n",
        "We'll install the following key libraries:\n",
        "- `datasets`: For loading and managing our training data\n",
        "- `langchain-couchbase`: To integrate Couchbase with LangChain for vector storage and caching\n",
        "- `langchain-google-vertexai`: For accessing Google VertexAI's embedding and chat models\n",
        "- `python-dotenv`: For securely managing environment variables and API keys\n",
        "\n",
        "These libraries provide the foundation for building a semantic search engine with vector embeddings,\n",
        "database integration, and agent-based RAG capabilities."
      ],
      "metadata": {
        "id": "NSuQ7AxaNPsh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N2cmqwh96l_"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet \\\n",
        "    \"google-cloud-aiplatform[langchain,agent_engines]\" requests datasets couchbase langchain langchain-community langchain-couchbase langchain-google-vertexai google-cloud-aiplatform langchain_google_genai requests beautifulsoup4 langsmith"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Necessary Libraries\n",
        "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading."
      ],
      "metadata": {
        "id": "AiU7di7-NJEH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woLoMPpf96l_"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.diagnostics import PingState, ServiceType\n",
        "from couchbase.exceptions import (InternalServerFailureException,\n",
        "                                  QueryIndexAlreadyExistsException,\n",
        "                                  ServiceUnavailableException)\n",
        "from couchbase.management.buckets import CreateBucketSettings\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Sensitive Information\n",
        "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like database credentials, and specific configuration names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
        "\n",
        "The script uses environment variables to store sensitive information, enhancing the overall security and maintainability of your code by avoiding hardcoded values."
      ],
      "metadata": {
        "id": "9mSH76U4NFYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHT3iN0k96l_"
      },
      "outputs": [],
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "CB_HOST = os.getenv('CB_HOST') or input(\"Enter Couchbase host (default: couchbase://localhost): \") or 'couchbase://localhost'\n",
        "CB_USERNAME = os.getenv('CB_USERNAME') or input(\"Enter Couchbase username (default: Administrator): \") or 'Administrator'\n",
        "CB_PASSWORD = os.getenv('CB_PASSWORD') or getpass.getpass(\"Enter Couchbase password (default: password): \") or 'password'\n",
        "CB_BUCKET_NAME = os.getenv('CB_BUCKET_NAME') or input(\"Enter bucket name (default: gemini): \") or 'gemini'\n",
        "INDEX_NAME = os.getenv('INDEX_NAME') or input(\"Enter index name (default: vector_search_gemini): \") or 'vector_search_gemini'\n",
        "SCOPE_NAME = os.getenv('SCOPE_NAME') or input(\"Enter scope name (default: _default): \") or '_default'\n",
        "COLLECTION_NAME = os.getenv('COLLECTION_NAME') or input(\"Enter collection name (default: _default): \") or '_default'\n",
        "\n",
        "print(\"Configuration loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting to the Couchbase Cluster\n",
        "Connecting to a Couchbase cluster is the foundation of our project. Couchbase will serve as our primary data store, handling all the storage and retrieval operations required for our semantic search engine. By establishing this connection, we enable our application to interact with the database, allowing us to perform operations such as storing embeddings, querying data, and managing collections. This connection is the gateway through which all data will flow, so ensuring it's set up correctly is paramount."
      ],
      "metadata": {
        "id": "vfwZQmx9NABd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-ZcG1q396l_"
      },
      "outputs": [],
      "source": [
        "# Connect to Couchbase\n",
        "try:\n",
        "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
        "    options = ClusterOptions(auth)\n",
        "    cluster = Cluster(CB_HOST, options)\n",
        "    cluster.wait_until_ready(timedelta(seconds=5))\n",
        "    print(\"Successfully connected to Couchbase\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to connect to Couchbase: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Collections in Couchbase\n",
        "\n",
        "The setup_collection() function handles creating and configuring the hierarchical data organization in Couchbase:\n",
        "\n",
        "1. Bucket Creation:\n",
        "   - Checks if specified bucket exists, creates it if not\n",
        "   - Sets bucket properties like RAM quota (1024MB) and replication (disabled)\n",
        "   - Note: If you are using Capella, create a bucket manually called vector-search-testing(or any name you prefer) with the same properties.\n",
        "\n",
        "2. Scope Management:  \n",
        "   - Verifies if requested scope exists within bucket\n",
        "   - Creates new scope if needed (unless it's the default \"_default\" scope)\n",
        "\n",
        "3. Collection Setup:\n",
        "   - Checks for collection existence within scope\n",
        "   - Creates collection if it doesn't exist\n",
        "   - Waits 2 seconds for collection to be ready\n",
        "\n",
        "Additional Tasks:\n",
        "- Creates primary index on collection for query performance\n",
        "- Clears any existing documents for clean state\n",
        "- Implements comprehensive error handling and logging\n",
        "\n",
        "The function is called twice to set up:\n",
        "1. Main collection for vector embeddings\n",
        "2. Cache collection for storing results\n"
      ],
      "metadata": {
        "id": "lpszgLu-_8JQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8Vk-r6296mA"
      },
      "outputs": [],
      "source": [
        "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
        "    try:\n",
        "        # Check if bucket exists, create if it doesn't\n",
        "        try:\n",
        "            bucket = cluster.bucket(bucket_name)\n",
        "            logging.info(f\"Bucket '{bucket_name}' exists.\")\n",
        "        except Exception as e:\n",
        "            logging.info(f\"Bucket '{bucket_name}' does not exist. Creating it...\")\n",
        "            bucket_settings = CreateBucketSettings(\n",
        "                name=bucket_name,\n",
        "                bucket_type='couchbase',\n",
        "                ram_quota_mb=1024,\n",
        "                flush_enabled=True,\n",
        "                num_replicas=0\n",
        "            )\n",
        "            cluster.buckets().create_bucket(bucket_settings)\n",
        "            bucket = cluster.bucket(bucket_name)\n",
        "            logging.info(f\"Bucket '{bucket_name}' created successfully.\")\n",
        "\n",
        "        bucket_manager = bucket.collections()\n",
        "\n",
        "        # Check if scope exists, create if it doesn't\n",
        "        scopes = bucket_manager.get_all_scopes()\n",
        "        scope_exists = any(scope.name == scope_name for scope in scopes)\n",
        "\n",
        "        if not scope_exists and scope_name != \"_default\":\n",
        "            logging.info(f\"Scope '{scope_name}' does not exist. Creating it...\")\n",
        "            bucket_manager.create_scope(scope_name)\n",
        "            logging.info(f\"Scope '{scope_name}' created successfully.\")\n",
        "\n",
        "        # Check if collection exists, create if it doesn't\n",
        "        collections = bucket_manager.get_all_scopes()\n",
        "        collection_exists = any(\n",
        "            scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
        "            for scope in collections\n",
        "        )\n",
        "\n",
        "        if not collection_exists:\n",
        "            logging.info(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
        "            bucket_manager.create_collection(scope_name, collection_name)\n",
        "            logging.info(f\"Collection '{collection_name}' created successfully.\")\n",
        "        else:\n",
        "            logging.info(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n",
        "\n",
        "        # Wait for collection to be ready\n",
        "        collection = bucket.scope(scope_name).collection(collection_name)\n",
        "        time.sleep(2)  # Give the collection time to be ready for queries\n",
        "\n",
        "        # Ensure primary index exists\n",
        "        try:\n",
        "            cluster.query(f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`\").execute()\n",
        "            logging.info(\"Primary index present or created successfully.\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error creating primary index: {str(e)}\")\n",
        "\n",
        "        # Clear all documents in the collection\n",
        "        try:\n",
        "            query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            cluster.query(query).execute()\n",
        "            logging.info(\"All documents cleared from the collection.\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n",
        "\n",
        "        return collection\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n",
        "\n",
        "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring and Initializing Couchbase Vector Search Index for Semantic Document Retrieval\n",
        "\n",
        "Semantic search requires an efficient way to retrieve relevant documents based on a user's query. This is where the Couchbase Vector Search Index comes into play. In this step, we load the Vector Search Index definition from a JSON file, which specifies how the index should be structured. This includes the fields to be indexed, the dimensions of the vectors, and other parameters that determine how the search engine processes queries based on vector similarity.\n",
        "\n",
        "This vector search index configuration requires specific default settings to function properly. This workshop uses the bucket named `gemini` with the scope `_default` and collection `_default`. The configuration is set up for vectors with exactly `768 dimensions`, using `dot product` similarity and optimized for `recall`. If you want to use a different bucket, scope, or collection, you will need to modify the index configuration accordingly.\n",
        "\n",
        "For more information on creating a vector search index, please follow the instructions at [Couchbase Vector Search Documentation](https://docs.couchbase.com/cloud/vector-search/create-vector-search-index-ui.html)."
      ],
      "metadata": {
        "id": "zkeI6xRaAAk9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfewOr5A96mA"
      },
      "outputs": [],
      "source": [
        "# Load index definition\n",
        "try:\n",
        "    with open('index.json', 'r') as file:\n",
        "        index_definition = json.load(file)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: crew_index.json file not found: {str(e)}\")\n",
        "    raise\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error: Invalid JSON in crew_index.json: {str(e)}\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"Error loading index definition: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating or Updating Search Indexes\n",
        "\n",
        "With the index definition loaded, the next step is to create or update the **Vector Search Index** in Couchbase. This step is crucial because it optimizes our database for vector similarity search operations, allowing us to perform searches based on the semantic content of documents rather than just keywords. By creating or updating a Vector Search Index, we enable our search engine to handle complex queries that involve finding semantically similar documents using vector embeddings, which is essential for a robust semantic search engine."
      ],
      "metadata": {
        "id": "0BAbK-FnAPZF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhGiWwr996mA"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    scope_index_manager = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME).search_indexes()\n",
        "\n",
        "    # Check if index already exists\n",
        "    existing_indexes = scope_index_manager.get_all_indexes()\n",
        "    index_name = index_definition[\"name\"]\n",
        "\n",
        "    if index_name in [index.name for index in existing_indexes]:\n",
        "        logging.info(f\"Index '{index_name}' found\")\n",
        "    else:\n",
        "        logging.info(f\"Creating new index '{index_name}'...\")\n",
        "\n",
        "    # Create SearchIndex object from JSON definition\n",
        "    search_index = SearchIndex.from_json(index_definition)\n",
        "\n",
        "    # Upsert the index (create if not exists, update if exists)\n",
        "    scope_index_manager.upsert_index(search_index)\n",
        "    logging.info(f\"Index '{index_name}' successfully created/updated.\")\n",
        "\n",
        "except QueryIndexAlreadyExistsException:\n",
        "    logging.info(f\"Index '{index_name}' already exists. Skipping creation/update.\")\n",
        "except ServiceUnavailableException:\n",
        "    raise RuntimeError(\"Search service is not available. Please ensure the Search service is enabled in your Couchbase cluster.\")\n",
        "except InternalServerFailureException as e:\n",
        "    logging.error(f\"Internal server error: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the Vertex AI SDK\n",
        "\n",
        "Run the following code in your notebook, replacing the placeholder values with your Google Cloud project ID, region, and staging bucket:"
      ],
      "metadata": {
        "id": "X6KVJcymAseN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"your_vertex_project_ID\"  # Replace with your project ID\n",
        "LOCATION = \"your_region\"         # Replace with your preferred region, e.g. \"us-central1\"\n",
        "STAGING_BUCKET = \"gs://your_bucket_name\"  # Replace with your bucket\n",
        "\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
      ],
      "metadata": {
        "id": "0EWcKczpAssK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingest Data into Couchbase\n",
        "Run the following code to scrape sample data from Wikipedia about Star Warss, convert the text into vector embeddings using the text-embedding-005 model, and then store this data in the corresponding collections in Couchbase\n",
        "."
      ],
      "metadata": {
        "id": "yWGB0JNmAaBK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATK5fe6Z96mA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import certifi\n",
        "import os\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "import uuid\n",
        "\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster, ClusterOptions\n",
        "from couchbase.collection import InsertOptions\n",
        "from couchbase.exceptions import CouchbaseException\n",
        "from datetime import timedelta  # ‚úÖ Required for wait_until_ready\n",
        "\n",
        "# Scrape website content\n",
        "def scrape_website(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    content = ' '.join([p.text for p in soup.find_all('p')])\n",
        "    return content\n",
        "\n",
        "# Split into chunks\n",
        "def split_into_chunks(text, chunk_size=1000):\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "# Get embeddings using GCP\n",
        "def get_text_embeddings(chunks):\n",
        "    model = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\n",
        "    embeddings = model.get_embeddings(chunks)\n",
        "    return [embedding.values for embedding in embeddings]\n",
        "\n",
        "# Store in Couchbase\n",
        "def write_to_couchbase(embeddings, chunks):\n",
        "    try:\n",
        "        cluster = Cluster(CB_HOST, ClusterOptions(\n",
        "            PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)))\n",
        "        cluster.wait_until_ready(timedelta(seconds=10))  # ‚úÖ Correct usage\n",
        "        bucket = cluster.bucket(CB_BUCKET_NAME)\n",
        "        collection = bucket.scope(SCOPE_NAME).collection(COLLECTION_NAME)\n",
        "\n",
        "        for i in range(len(chunks)):\n",
        "            doc_id = f\"{COLLECTION_NAME}_{uuid.uuid4()}\"\n",
        "            doc = {\n",
        "                \"text\": chunks[i],\n",
        "                \"embedding\": embeddings[i]\n",
        "            }\n",
        "            collection.insert(doc_id, doc, InsertOptions(timeout=timedelta(seconds=5)))\n",
        "            print(f\"üì• Inserted document ID: {doc_id}\")\n",
        "\n",
        "    except CouchbaseException as e:\n",
        "        print(f\"‚ùå Error inserting into Couchbase: {e}\")\n",
        "\n",
        "# Ingest content from a URL\n",
        "def ingest_url(url, label):\n",
        "    print(f\"\\nüîç Processing: {label}\")\n",
        "    content = scrape_website(url)\n",
        "    chunks = split_into_chunks(content)\n",
        "    embeddings = get_text_embeddings(chunks)\n",
        "    write_to_couchbase(embeddings, chunks)\n",
        "\n",
        "# Run ingestion for Wikipedia sources\n",
        "ingest_url(\"https://en.wikipedia.org/wiki/Star_Wars\", \"Star Wars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Agent\n",
        "\n",
        "In this section, you define tools that the agent can use to query your collections using Couchase Vector Search, create a memory system to maintain conversation context, and then initialize the agent using LangChain."
      ],
      "metadata": {
        "id": "TpF3Mh3sAiHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define tools for the agent\n",
        "\n",
        "### 1. Star Wars Query Tool\n",
        "\n",
        "Run the following code to create a tool that uses Couchbase Vector Search to query the starwars data from the _default collection:"
      ],
      "metadata": {
        "id": "15IsP-OABC3B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y5aE39F96mA"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "def star_wars_query_tool(\n",
        "    query: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieves vectors from a Couchbase database and uses them to answer a question related to Star wars.\n",
        "\n",
        "    Args:\n",
        "        query: The question to be answered about star wars.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the response to the question.\n",
        "    \"\"\"\n",
        "    from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "    from langchain_couchbase import CouchbaseVectorStore\n",
        "    from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "    from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from couchbase.auth import PasswordAuthenticator\n",
        "    from couchbase.cluster import Cluster\n",
        "    from couchbase.options import ClusterOptions\n",
        "    from datetime import timedelta\n",
        "    import os\n",
        "\n",
        "    # Create local Couchbase connection to avoid serialization issues\n",
        "    auth = PasswordAuthenticator(\n",
        "        os.getenv('CB_USERNAME', 'pdf'),\n",
        "        os.getenv('CB_PASSWORD', 'Pdf1234!')\n",
        "    )\n",
        "    options = ClusterOptions(auth)\n",
        "    local_cluster = Cluster(os.getenv('CB_HOST', 'couchbases://cb.hxdufdz4y5boruny.cloud.couchbase.com'), options)\n",
        "    local_cluster.wait_until_ready(timedelta(seconds=5))\n",
        "\n",
        "\n",
        "    embeddings = VertexAIEmbeddings(model_name=\"text-embedding-005\")\n",
        "\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Do not return any answers from your own knowledge. Respond only in 2 or 3 sentences.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "     Also add the {context} to the response.\n",
        "    \"\"\"\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    vs = CouchbaseVectorStore(\n",
        "        cluster=local_cluster,\n",
        "        bucket_name=os.getenv('CB_BUCKET_NAME', 'gemini'),\n",
        "        scope_name=os.getenv('SCOPE_NAME', '_default'),\n",
        "        collection_name=os.getenv('COLLECTION_NAME', '_default'),\n",
        "        embedding=embeddings,\n",
        "        index_name=os.getenv('INDEX_NAME', 'gemini_vector'),\n",
        "        text_key=\"text\",  # Use \"text\" since that's what you stored in your documents\n",
        "    )\n",
        "    llm = ChatVertexAI(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        convert_system_message_to_human=True,\n",
        "        max_output_tokens=1000,\n",
        "        temperature=0.1,\n",
        "    )\n",
        "    retriever = vs.as_retriever()\n",
        "\n",
        "    memory = ConversationBufferWindowMemory(\n",
        "        memory_key=\"chat_history\", k=5, return_messages=True\n",
        "    )\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        "    )\n",
        "    response = conversation_chain({\"question\": query})\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Blog Writing Tool\n",
        "\n",
        "Run the following code to create a tool that writes a well structured blog post.\n",
        "\n",
        "   - Takes the research summary as input\n",
        "   - Structures and formats the information\n",
        "   - Creates a polished, user-friendly response\n",
        "   - Ensures proper attribution and citation\n",
        "\n"
      ],
      "metadata": {
        "id": "QorW07zPBXZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq21mZFq96mA"
      },
      "outputs": [],
      "source": [
        "def blog_writing_tool(\n",
        "    research_context: str,\n",
        "    topic: str = \"Star Wars\",\n",
        "    blog_style: str = \"informative\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a well-structured blog post based on research context about Star Wars.\n",
        "\n",
        "    Args:\n",
        "        research_context: The research information to base the blog post on\n",
        "        topic: The main topic for the blog post (default: \"Star Wars\")\n",
        "        blog_style: The style of the blog post (informative, casual, academic, etc.)\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the generated blog post\n",
        "    \"\"\"\n",
        "    from langchain_google_vertexai import ChatVertexAI\n",
        "    from langchain.prompts import PromptTemplate\n",
        "\n",
        "    # Blog writing prompt template\n",
        "    blog_prompt_template = \"\"\"You are an expert blog writer specializing in {topic} content.\n",
        "    Based on the research context provided, write a well-structured, engaging blog post.\n",
        "\n",
        "    Research Context:\n",
        "    {research_context}\n",
        "\n",
        "    Please write a blog post with the following requirements:\n",
        "    - Style: {blog_style}\n",
        "    - Include a compelling title\n",
        "    - Have clear sections with headings\n",
        "    - Be approximately 500-800 words\n",
        "    - Include an engaging introduction and conclusion\n",
        "    - Use the research context as the primary source of information\n",
        "    - Make it SEO-friendly with relevant keywords\n",
        "    - Write in a {blog_style} tone\n",
        "\n",
        "    Format the output as:\n",
        "    # [Blog Title]\n",
        "\n",
        "    ## Introduction\n",
        "    [Introduction content]\n",
        "\n",
        "    ## [Section 1 Heading]\n",
        "    [Section 1 content]\n",
        "\n",
        "    ## [Section 2 Heading]\n",
        "    [Section 2 content]\n",
        "\n",
        "    ## Conclusion\n",
        "    [Conclusion content]\n",
        "\n",
        "    Blog Post:\"\"\"\n",
        "\n",
        "    BLOG_PROMPT = PromptTemplate(\n",
        "        template=blog_prompt_template,\n",
        "        input_variables=[\"research_context\", \"topic\", \"blog_style\"]\n",
        "    )\n",
        "\n",
        "    # Initialize the LLM for blog writing\n",
        "    llm = ChatVertexAI(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        convert_system_message_to_human=True,\n",
        "        max_output_tokens=2000,\n",
        "        temperature=0.7,  # Higher temperature for more creative writing\n",
        "    )\n",
        "\n",
        "    # Generate the blog post\n",
        "    formatted_prompt = BLOG_PROMPT.format(\n",
        "        research_context=research_context,\n",
        "        topic=topic,\n",
        "        blog_style=blog_style\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "\n",
        "    return {\n",
        "        \"blog_post\": response.content,\n",
        "        \"topic\": topic,\n",
        "        \"style\": blog_style,\n",
        "        \"word_count\": len(response.content.split())\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a memory system\n",
        "\n",
        "You can use LangChain to create memory for your agent so that it can maintain conversation context across multiple prompts:"
      ],
      "metadata": {
        "id": "4WT7EiXaK5L3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zlv-mc296mA"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "# Initialize session history\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "  if session_id not in store:\n",
        "    store[session_id] = ChatMessageHistory()\n",
        "  return store[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the agent\n",
        "\n",
        "Create the agent using LangChain. This agent uses the tools and memory system that you defined."
      ],
      "metadata": {
        "id": "p1jC8Su_K8Uk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieKYDAul96mA"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.reasoning_engines import LangchainAgent\n",
        "\n",
        "unified_multi_agent = LangchainAgent(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    chat_history=get_session_history,\n",
        "    model_kwargs={\"temperature\": 0.3},  # Balanced temperature\n",
        "    tools=[\n",
        "        star_wars_query_tool,          # Research tool\n",
        "        blog_writing_tool,             # Blog writing tool\n",
        "    ],\n",
        "    agent_executor_kwargs={\"return_intermediate_steps\": True},\n",
        "    system_instruction=\"\"\"You are a multi-agent AI system specialized in Star Wars content research and blog creation.\n",
        "\n",
        "You have access to two main tools that you can chain together:\n",
        "1. star_wars_query_tool: For researching Star Wars topics using vector database\n",
        "2. blog_writing_tool: For creating well-structured blog posts from research content\n",
        "\n",
        "üîó TOOL CHAINING INSTRUCTIONS:\n",
        "\n",
        "FOR BLOG POST REQUESTS:\n",
        "When a user asks for a blog post about a Star Wars topic, follow these steps:\n",
        "1. FIRST: Use star_wars_query_tool to research the topic and gather context\n",
        "2. THEN: Use blog_writing_tool with the research context from step 1 as the 'research_context' parameter\n",
        "\n",
        "FOR RESEARCH-ONLY REQUESTS:\n",
        "When a user asks for information/research only, use star_wars_query_tool directly.\n",
        "\n",
        "FOR BLOG WITH PROVIDED CONTEXT:\n",
        "When a user provides their own research/context, use blog_writing_tool directly.\n",
        "\n",
        "üéØ EXAMPLE WORKFLOW FOR BLOG CREATION:\n",
        "User: \"Write a blog post about Darth Vader\"\n",
        "1. Call star_wars_query_tool(\"Darth Vader journey character development\")\n",
        "2. Take the 'answer' from the result\n",
        "3. Call blog_writing_tool(research_context=<answer from step 2>, topic=\"Darth Vader\", blog_style=\"informative\")\n",
        "4. Return the generated blog post\n",
        "\n",
        "Always:\n",
        "- Provide detailed, accurate, and engaging responses\n",
        "- Use the research context as the primary source for blog posts\n",
        "- Maintain professional quality in all outputs\n",
        "- Show your reasoning process when chaining tools\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the agent with a sample query:"
      ],
      "metadata": {
        "id": "5AM_3nsaLdkG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubdFSu-y96mA"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Test the unified agent with different use cases\n",
        "test_cases = [\n",
        "    {\n",
        "        \"query\": \"Create a complete blog post about Darth Vader's journey from Anakin Skywalker\",\n",
        "        \"description\": \"Complete blog creation (research + writing)\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"While filming the original 1977 film, What was the amount of pay cut from his salary that George Lucas decided to takeas director in exchange for full ownership of the franchise's merchandising rights.\",\n",
        "        \"description\": \"Research only\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, test_case in enumerate(test_cases, 1):\n",
        "    print(f\"\\nüìã TEST CASE {i}: {test_case['description']}\")\n",
        "    print(f\"üîç Query: {test_case['query']}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    try:\n",
        "        response = unified_multi_agent.query(\n",
        "            input=test_case['query'],\n",
        "            config={\"configurable\": {\"session_id\": f\"unified_test_{i}\"}}\n",
        "        )\n",
        "\n",
        "        output = response.get(\"output\", \"\")\n",
        "        print(f\"Response length: {len(output)} characters\")\n",
        "        print(f\" Response preview: {output[:200]}...\")\n",
        "\n",
        "        # Display full response if it's a blog post\n",
        "        if \"blog\" in test_case['query'].lower():\n",
        "            display(Markdown(\"### Generated Blog Post:\"))\n",
        "            display(Markdown(output))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error: {str(e)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy the Agent\n",
        "In this section, you deploy your agent to the Vertex AI Agent Engine as a managed service. This allows you to scale your agent and use it in production without managing the underlying infrastructure."
      ],
      "metadata": {
        "id": "oQ_6frY1LhX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy your agent.\n",
        "Run the following code to configure and deploy the agent in the Vertex AI Agent Engine:"
      ],
      "metadata": {
        "id": "KIKhO5B4Li_P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFfTlMy796mB"
      },
      "outputs": [],
      "source": [
        "from vertexai import agent_engines\n",
        "\n",
        "try:\n",
        "    # Deploy the two-tool chained multi-agent system\n",
        "    deployed_multi_agent = agent_engines.create(\n",
        "        unified_multi_agent,\n",
        "        requirements=[\n",
        "            \"google-cloud-aiplatform[agent_engines,langchain]\",\n",
        "            \"cloudpickle==3.0.0\",\n",
        "            \"pydantic>=2.10\",\n",
        "            \"requests\",\n",
        "            \"langchain-couchbase\",\n",
        "            \"pymongo\",\n",
        "            \"langchain-google-vertexai\",\n",
        "            \"beautifulsoup4\",  # For web scraping in research\n",
        "        ],\n",
        "        display_name=\"StarWars-TwoTool-ChainedAgent\",\n",
        "        description=\"Two-tool chained agent for Star Wars research and blog writing - automatically chains research and writing tools\"\n",
        "    )\n",
        "\n",
        "    print(\"Two-Tool Chained Agent deployed successfully!\")\n",
        "    print(f\"üìç Resource Name: {deployed_multi_agent.resource_name}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Deployment failed: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve the project URL.\n",
        "Run the following code to retrieve the project number associated with your project ID. This project number will be used to construct the complete resource name for your deployed agent:"
      ],
      "metadata": {
        "id": "6UMVPw-7L5Zo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG35Q-qa96mB"
      },
      "outputs": [],
      "source": [
        "from googleapiclient import discovery\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Retrieve the project number associated with your project ID\n",
        "service = discovery.build(\"cloudresourcemanager\", \"v1\")\n",
        "request = service.projects().get(projectId=PROJECT_ID)\n",
        "response = request.execute()\n",
        "project_number = response[\"projectNumber\"]\n",
        "print(f\"Project Number: {project_number}\")\n",
        "# The deployment creates a unique ID for your agent that you can find in the output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the agent.\n",
        "Run the following code to use your agent. Replace the placeholder with your agent's full resource name:"
      ],
      "metadata": {
        "id": "gAYzykNiL9qf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW37PGj196mB"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview import reasoning_engines\n",
        "\n",
        "# Replace with your agent's full resource name from the previous step\n",
        "REASONING_ENGINE_RESOURCE_NAME = \"<resource-name>\"\n",
        "\n",
        "remote_agent = reasoning_engines.ReasoningEngine(REASONING_ENGINE_RESOURCE_NAME)\n",
        "\n",
        "response = remote_agent.query(\n",
        "    input=\"While filming the original 1977 film, What was the amount of pay cut from his salary that George Lucas decided to takeas director in exchange for full ownership of the franchise's merchandising rights.\",\n",
        "    config={\"configurable\": {\"session_id\": \"demo\"}},\n",
        ")\n",
        "print(response[\"output\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing another test to write a blog with the remote deployed agent"
      ],
      "metadata": {
        "id": "0uF26FUEMKzk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO-6fwDX96mB"
      },
      "outputs": [],
      "source": [
        "# Single Test Case: Blog Writing with Remote Unified Agent\n",
        "\n",
        "from vertexai.preview import reasoning_engines\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Use the deployed unified multi-agent system\n",
        "UNIFIED_AGENT_RESOURCE_NAME = \"<resource-name>\"\n",
        "\n",
        "remote_unified_agent = reasoning_engines.ReasoningEngine(UNIFIED_AGENT_RESOURCE_NAME)\n",
        "\n",
        "print(\"üìù Testing Blog Creation with Remote Unified Agent\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Single focused test case for blog writing\n",
        "blog_query = \"While filming the original 1977 film, What was the amount of pay cut from his salary that George Lucas decided to takeas director in exchange for full ownership of the franchise's merchandising rights.\"\n",
        "\n",
        "print(f\"üîç Query: {blog_query}\")\n",
        "print(f\"üéØ Expected: Complete blog post with research and writing\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "try:\n",
        "    # Execute the blog creation request\n",
        "    response = remote_unified_agent.query(\n",
        "        input=blog_query,\n",
        "        config={\"configurable\": {\"session_id\": \"blog_test\"}}\n",
        "    )\n",
        "\n",
        "    output = response.get(\"output\", \"\")\n",
        "\n",
        "    print(f\"‚úÖ Blog post generated: {len(output)} characters\")\n",
        "\n",
        "    # Display the generated blog post\n",
        "    if output:\n",
        "        print(f\"üì∞ Blog Post Preview: {output[:200]}...\")\n",
        "        print(\"\\n\" + \"=\"*55)\n",
        "        print(\"üìñ FULL BLOG POST:\")\n",
        "        print(\"=\"*55)\n",
        "        display(Markdown(output))\n",
        "    else:\n",
        "        print(\"‚ùå No content generated\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {str(e)}\")\n",
        "\n",
        "print(\"\\nüéâ Blog creation test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "You can also debug and optimize your agents by enabling [tracing](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/manage/tracing) in the Agent Engine. Refer to the [Vertex AI Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/) documentation for other features and examples.\n"
      ],
      "metadata": {
        "id": "CPBBvSb9MiZv"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}